{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning for COVID-19 Detection in X-Ray Images\n",
    "## DD2424 Deep Learning - Group Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter difference between current generated dataset and paper dataset to get the correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    file_lines = [x.strip() for x in content]\n",
    "    return file_lines\n",
    "\n",
    "def filter_out_difference(correct_data_description, wrong_data_description):\n",
    "    correct_file_lines = get_file_lines(correct_data_description)\n",
    "    wrong_file_lines = get_file_lines(wrong_data_description)\n",
    "    \n",
    "    nr_missing_images = 0\n",
    "    \n",
    "    # Need to check substrings as wrong_file_description has longer lines\n",
    "    for correct_line in correct_file_lines:\n",
    "        if not any(correct_line in line for line in wrong_file_lines):\n",
    "            nr_missing_images += 1\n",
    "                \n",
    "    if nr_missing_images > 0:\n",
    "        print('you are missing: ', nr_missing_images, 'images')\n",
    "        return\n",
    "    print('you have all images in ', correct_data_description)\n",
    "    \n",
    "    new_correct_upd_lines = []\n",
    "    \n",
    "    for wrong_line in wrong_file_lines:\n",
    "        for correct_line in correct_file_lines:\n",
    "            if correct_line in wrong_line:\n",
    "                new_correct_upd_lines.append(wrong_line)\n",
    "                   \n",
    "    return new_correct_upd_lines\n",
    "    \n",
    "def write_line_list_to_file(file, line_list):\n",
    "    with open(file, 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % line for line in line_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_test_data_description = 'data/paper_dataset_specifications/test_COVIDx2.txt'\n",
    "paper_train_data_description = 'data/paper_dataset_specifications/train_COVIDx2.txt'\n",
    "\n",
    "current_test_data_description = 'data/test_split_v3.txt'\n",
    "current_train_data_description = 'data/train_split_v3.txt'\n",
    "\n",
    "upd_test_set_list = filter_out_difference(paper_test_data_description, current_test_data_description)\n",
    "upd_train_set_list = filter_out_difference(paper_train_data_description, current_train_data_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing updated and correct dataset description to txt file\n",
    "# write_line_list_to_file('data/correct_test_split.txt',upd_test_set_list)\n",
    "# write_line_list_to_file('data/correct_train_split.txt',upd_train_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving images to class partitioned directory\n",
    "mapping = {\n",
    "            'normal': 0,\n",
    "            'pneumonia': 1,\n",
    "            'COVID-19': 2}\n",
    "\n",
    "def get_label(img_desc_list):\n",
    "        for class_name in mapping:\n",
    "            if class_name in img_desc_list:\n",
    "                return mapping[class_name]\n",
    "\n",
    "current_dir = 'data/dataset/test/test_data'\n",
    "new_dir = 'data/dataset/test/class'\n",
    "img_descriptions = get_file_lines('data/correct_test_split.txt')\n",
    "\n",
    "\n",
    "data_path = os.path.join(current_dir, '*g')\n",
    "images = glob.glob(data_path)\n",
    "\n",
    "for image_path in images:\n",
    "            image_name = image_path.replace(current_dir + '/', '')\n",
    "            for img_desc in img_descriptions:\n",
    "                if image_name in img_desc:\n",
    "                    img_desc_list = img_desc.split()\n",
    "                    label = get_label(img_desc_list)\n",
    "                    os.rename(image_path, new_dir +str(label) + '/' + image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for handling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of this class in unnecessary now adays\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            test_img_dir,\n",
    "            train_img_dir,\n",
    "            test_img_descriptions_file,\n",
    "            train_img_descriptions_file,\n",
    "            input_shape = (224, 224),\n",
    "            batch_size = 10\n",
    "                 ):\n",
    "        self.test_img_dir = test_img_dir\n",
    "        self.train_img_dir = train_img_dir\n",
    "        self.test_img_descriptions = get_file_lines(test_img_descriptions_file)\n",
    "        self.train_img_descriptions = get_file_lines(train_img_descriptions_file)\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_nr = 1\n",
    "        self.max_batch = len(self.train_img_descriptions) // self.batch_size\n",
    "        self.mapping = {\n",
    "                'normal': 0,\n",
    "                'pneumonia': 1,\n",
    "                'COVID-19': 2}\n",
    "\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.x_batch = None\n",
    "        self.y_batch = None\n",
    "\n",
    "    \n",
    "    def get_current_batch(self):\n",
    "        return self.x_batch, self.y_batch\n",
    "    \n",
    "    def _get_class_dist(self, y):\n",
    "        class_dist = {}\n",
    "        for class_name in self.mapping:\n",
    "            class_dist[class_name] = np.count_nonzero(y == self.mapping[class_name])\n",
    "        return class_dist\n",
    "\n",
    "    def get_test_class_dist(self):\n",
    "        test_class_dist = self._get_class_dist(self.y_test)\n",
    "\n",
    "        return test_class_dist\n",
    "\n",
    "    def read_test_data(self):\n",
    "        self.x_test, self.y_test = self._read_correct_images(self.test_img_dir, self.test_img_descriptions)\n",
    "\n",
    "    def _get_label(self, img_desc_list):\n",
    "        for class_name in self.mapping:\n",
    "            if class_name in img_desc_list:\n",
    "                return self.mapping[class_name]\n",
    "            \n",
    "    # Must load training data in batches since memory error otherwise    \n",
    "    def next_train_batch(self):\n",
    "        \n",
    "        if self.batch_nr == self.max_batch:\n",
    "            print('No data left')\n",
    "            return [], []\n",
    "        \n",
    "        start_img = (self.batch_nr-1) * self.batch_size\n",
    "        end_img = (self.batch_nr) * self.batch_size        \n",
    "                \n",
    "        batch_descriptions = self.train_img_descriptions[start_img: end_img]\n",
    "        \n",
    "        self.x_batch, self.y_batch = self._read_correct_images(self.train_img_dir, batch_descriptions)\n",
    "        \n",
    "        self.batch_nr += 1\n",
    "        \n",
    "\n",
    "    def _read_correct_images(self, img_dir, img_descriptions):\n",
    "        data_path = os.path.join(img_dir, '*g')\n",
    "        images = glob.glob(data_path)\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for image_path in images:\n",
    "            image_name = image_path.replace(img_dir + '/', '')\n",
    "            for img_desc in img_descriptions:\n",
    "                if image_name in img_desc:\n",
    "                    img_desc_list = img_desc.split()\n",
    "                    label = self._get_label(img_desc_list)\n",
    "                    y_list.append(label)\n",
    "\n",
    "                    img_array = cv2.imread(image_path)\n",
    "                    resized_img_array = cv2.resize(img_array, self.input_shape)\n",
    "                    x_list.append(resized_img_array)\n",
    "                    \n",
    "        y_array = np.array(y_list)\n",
    "        x_array = np.array(x_list)        \n",
    "\n",
    "        return x_array, y_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'data/dataset/test'\n",
    "test_data_description = 'data/correct_test_split.txt'\n",
    "\n",
    "train_data = 'data/dataset/train'\n",
    "train_data_description = 'data/correct_train_split.txt'\n",
    "\n",
    "dataset = Dataset(\n",
    "        test_data,\n",
    "        train_data,\n",
    "        test_data_description,\n",
    "        train_data_description\n",
    "       )\n",
    "\n",
    "# Read test data\n",
    "dataset.read_test_data()\n",
    "\n",
    "print(dataset.get_test_class_dist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of concept with simple ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing model with freezed weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image directories, constants, use of generator and use of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image import/process can be done using ImageDataGenerator, which enables easy data augmentation if we wish.\n",
    "If more suitable, put use_generator=false and specify all x and y\"\"\" \n",
    "\n",
    "# constants as in report\n",
    "img_size = 224 # double check in report\n",
    "learning_rate=2e-5\n",
    "epochs = 3 # in report 22\n",
    "batch_size = 8\n",
    "factor=0.7 # used in callback\n",
    "patience = 5 # used in callback\n",
    "optimizer='Adam'\n",
    "\n",
    "# adjust for number of samples\n",
    "nb_train_samples = 20\n",
    "nb_val_samples = 12\n",
    "\n",
    "# specify use of validation and/or generator feature\n",
    "use_generator = True\n",
    "use_validation = True \n",
    "use_callbacks = False\n",
    "\n",
    "if use_generator:\n",
    "    train_dir = 'data/dataset/johanna_testdata/train'\n",
    "    val_dir = 'data/dataset/johanna_testdata/val'\n",
    "    test_dir = 'data/dataset/johanna_testdata/test'\n",
    "    \n",
    "if not use_generator: \n",
    "    x_train, y_train = None, None\n",
    "    x_test, y_test = None, None\n",
    "    val_data = None \n",
    "    \n",
    "if use_callbacks: \n",
    "    callbacks = [ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=factor, \n",
    "    patience=patience, \n",
    "    verbose=0)]\n",
    "\n",
    "if not use_callbacks:\n",
    "    callbacks = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating model and freezing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 41,458,499\n",
      "Trainable params: 33,823,235\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model and freeze all but last conv block\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# model = \"the shell\", adding VGG network and extra layers to model\n",
    "model = models.Sequential() \n",
    "model.add(vgg_conv)\n",
    "model.add(layers.Flatten()) # in the report, this results in output shape 100,352 instead of 25,088\n",
    "model.add(layers.Dense(1024))\n",
    "model.add(layers.Dense(1024))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# summerize and compile\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run if ImageDataGenerator() is used to process images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 3 classes.\n",
      "Found 9 images belonging to 3 classes.\n",
      "Found 12 images belonging to 3 classes.\n",
      "Generators generated :-)\n",
      "Model fine-tuned\n",
      "Model evaluated\n"
     ]
    }
   ],
   "source": [
    "# Rescale=1./255 --> normalizing pixel values to be in range 0-1 (max pixel value is 255). Our pictures are\n",
    "# grayscale (1 dimension), since VGG requires RGB (3 dimensions), the generators will copy our 1D data to every\n",
    "# dimension => (PixelValue, PixelValue, PixelValue). This is Okay.\n",
    "if use_generator: \n",
    "    datagen = ImageDataGenerator(Rescale=1./255)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "    test_generator = datagen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "    if use_validation:\n",
    "        val_generator = datagen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "    else:\n",
    "        val_generator = None\n",
    "        \n",
    "    print(\"Generators generated :-)\")\n",
    "    \n",
    "    # fine-tune model\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=nb_val_samples // batch_size, \n",
    "        verbose=0, \n",
    "        callbacks = callbacks)\n",
    "    \n",
    "    print(\"Model fine-tuned\")\n",
    "    \n",
    "    # evaluate model\n",
    "    result = model.evaluate_generator(test_generator)\n",
    "    \n",
    "    print(\"Model evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run if ImageDataGenerator is NOT used to process images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_generator:\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        validation_data=val_data,\n",
    "        validation_steps= nb_val_samples // batch_size,\n",
    "        verbose=0\n",
    "        callbacks = [callback])\n",
    "    \n",
    "    result = model.evaluate(\n",
    "        x=x_test, \n",
    "        y=y_test, \n",
    "        batch_size=batch_size, \n",
    "        verbose=0)\n",
    "else: \n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print accuracy for model with frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "accuracy 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "print(model.metrics_names[1], result[1])\n",
    "#print(model.metrics_names[2], result[2])\n",
    "#prediction = model.predict_generator(test_generator)\n",
    "#print(prediction)\n",
    "#model.save('transfer_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing model with no frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}