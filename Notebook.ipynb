{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning for COVID-19 Detection in X-Ray Images\n",
    "## DD2424 Deep Learning - Group Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter difference between current generated dataset and paper dataset to get the correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    file_lines = [x.strip() for x in content]\n",
    "    return file_lines\n",
    "\n",
    "def filter_out_difference(correct_data_description, wrong_data_description):\n",
    "    correct_file_lines = get_file_lines(correct_data_description)\n",
    "    wrong_file_lines = get_file_lines(wrong_data_description)\n",
    "    \n",
    "    nr_missing_images = 0\n",
    "    \n",
    "    # Need to check substrings as wrong_file_description has longer lines\n",
    "    for correct_line in correct_file_lines:\n",
    "        if not any(correct_line in line for line in wrong_file_lines):\n",
    "            nr_missing_images += 1\n",
    "                \n",
    "    if nr_missing_images > 0:\n",
    "        print('you are missing: ', nr_missing_images, 'images')\n",
    "        return\n",
    "    print('you have all images in ', correct_data_description)\n",
    "    \n",
    "    new_correct_upd_lines = []\n",
    "    \n",
    "    for wrong_line in wrong_file_lines:\n",
    "        for correct_line in correct_file_lines:\n",
    "            if correct_line in wrong_line:\n",
    "                new_correct_upd_lines.append(wrong_line)\n",
    "                   \n",
    "    return new_correct_upd_lines\n",
    "    \n",
    "def write_line_list_to_file(file, line_list):\n",
    "    with open(file, 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % line for line in line_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_test_data_description = 'data/paper_dataset_specifications/test_COVIDx2.txt'\n",
    "paper_train_data_description = 'data/paper_dataset_specifications/train_COVIDx2.txt'\n",
    "\n",
    "current_test_data_description = 'data/test_split_v3.txt'\n",
    "current_train_data_description = 'data/train_split_v3.txt'\n",
    "\n",
    "upd_test_set_list = filter_out_difference(paper_test_data_description, current_test_data_description)\n",
    "upd_train_set_list = filter_out_difference(paper_train_data_description, current_train_data_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing updated and correct dataset description to txt file\n",
    "# write_line_list_to_file('data/correct_test_split.txt',upd_test_set_list)\n",
    "# write_line_list_to_file('data/correct_train_split.txt',upd_train_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving images to class partitioned directory\n",
    "mapping = {\n",
    "            'normal': 0,\n",
    "            'pneumonia': 1,\n",
    "            'COVID-19': 2}\n",
    "\n",
    "def get_label(img_desc_list):\n",
    "        for class_name in mapping:\n",
    "            if class_name in img_desc_list:\n",
    "                return mapping[class_name]\n",
    "\n",
    "current_dir = 'data/dataset/excess_train'\n",
    "new_dir = 'data/dataset/val/class'\n",
    "img_descriptions = get_file_lines('data/old_wrong_train_split_v3.txt')\n",
    "\n",
    "\n",
    "data_path = os.path.join(current_dir, '*g')\n",
    "images = glob.glob(data_path)\n",
    "\n",
    "for image_path in images:\n",
    "            image_name = image_path.replace(current_dir + '/', '')\n",
    "            for img_desc in img_descriptions:\n",
    "                if image_name in img_desc:\n",
    "                    img_desc_list = img_desc.split()\n",
    "                    label = get_label(img_desc_list)\n",
    "                    os.rename(image_path, new_dir +str(label) + '/' + image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for handling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of this class in unnecessary now adays\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            test_img_dir,\n",
    "            train_img_dir,\n",
    "            test_img_descriptions_file,\n",
    "            train_img_descriptions_file,\n",
    "            input_shape = (224, 224),\n",
    "            batch_size = 10\n",
    "                 ):\n",
    "        self.test_img_dir = test_img_dir\n",
    "        self.train_img_dir = train_img_dir\n",
    "        self.test_img_descriptions = get_file_lines(test_img_descriptions_file)\n",
    "        self.train_img_descriptions = get_file_lines(train_img_descriptions_file)\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_nr = 1\n",
    "        self.max_batch = len(self.train_img_descriptions) // self.batch_size\n",
    "        self.mapping = {\n",
    "                'normal': 0,\n",
    "                'pneumonia': 1,\n",
    "                'COVID-19': 2}\n",
    "\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.x_batch = None\n",
    "        self.y_batch = None\n",
    "\n",
    "    \n",
    "    def get_current_batch(self):\n",
    "        return self.x_batch, self.y_batch\n",
    "    \n",
    "    def _get_class_dist(self, y):\n",
    "        class_dist = {}\n",
    "        for class_name in self.mapping:\n",
    "            class_dist[class_name] = np.count_nonzero(y == self.mapping[class_name])\n",
    "        return class_dist\n",
    "\n",
    "    def get_test_class_dist(self):\n",
    "        test_class_dist = self._get_class_dist(self.y_test)\n",
    "\n",
    "        return test_class_dist\n",
    "\n",
    "    def read_test_data(self):\n",
    "        self.x_test, self.y_test = self._read_correct_images(self.test_img_dir, self.test_img_descriptions)\n",
    "\n",
    "    def _get_label(self, img_desc_list):\n",
    "        for class_name in self.mapping:\n",
    "            if class_name in img_desc_list:\n",
    "                return self.mapping[class_name]\n",
    "            \n",
    "    # Must load training data in batches since memory error otherwise    \n",
    "    def next_train_batch(self):\n",
    "        \n",
    "        if self.batch_nr == self.max_batch:\n",
    "            print('No data left')\n",
    "            return [], []\n",
    "        \n",
    "        start_img = (self.batch_nr-1) * self.batch_size\n",
    "        end_img = (self.batch_nr) * self.batch_size        \n",
    "                \n",
    "        batch_descriptions = self.train_img_descriptions[start_img: end_img]\n",
    "        \n",
    "        self.x_batch, self.y_batch = self._read_correct_images(self.train_img_dir, batch_descriptions)\n",
    "        \n",
    "        self.batch_nr += 1\n",
    "        \n",
    "\n",
    "    def _read_correct_images(self, img_dir, img_descriptions):\n",
    "        data_path = os.path.join(img_dir, '*g')\n",
    "        images = glob.glob(data_path)\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for image_path in images:\n",
    "            image_name = image_path.replace(img_dir + '/', '')\n",
    "            for img_desc in img_descriptions:\n",
    "                if image_name in img_desc:\n",
    "                    img_desc_list = img_desc.split()\n",
    "                    label = self._get_label(img_desc_list)\n",
    "                    y_list.append(label)\n",
    "\n",
    "                    img_array = cv2.imread(image_path)\n",
    "                    resized_img_array = cv2.resize(img_array, self.input_shape)\n",
    "                    x_list.append(resized_img_array)\n",
    "                    \n",
    "        y_array = np.array(y_list)\n",
    "        x_array = np.array(x_list)        \n",
    "\n",
    "        return x_array, y_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal': 100, 'pneumonia': 100, 'COVID-19': 31}\n"
     ]
    }
   ],
   "source": [
    "test_data = 'data/dataset/test'\n",
    "test_data_description = 'data/correct_test_split.txt'\n",
    "\n",
    "train_data = 'data/dataset/train'\n",
    "train_data_description = 'data/correct_train_split.txt'\n",
    "\n",
    "dataset = Dataset(\n",
    "        test_data,\n",
    "        train_data,\n",
    "        test_data_description,\n",
    "        train_data_description\n",
    "       )\n",
    "\n",
    "# Read test data\n",
    "dataset.read_test_data()\n",
    "\n",
    "print(dataset.get_test_class_dist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of concept with simple ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing images from folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 13569\n",
      "validation samples: 1350\n",
      "Found 13569 images belonging to 3 classes.\n",
      "Found 1350 images belonging to 3 classes.\n",
      "Found 231 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Image import: This was mainly to see if I could get the model working, ImageDataGenerator might not be the best choice.\n",
    "However, if we want to do data augmentation later, these function are \n",
    "built-in here. To run the script, you need 3 folder: dataset/data/train, dataset/data/val, dataset/data/test. The first two should have \n",
    "three subfolders each: class1, class2, class3 with corresponding pictures\"\"\"\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# folders\n",
    "train_dir = 'data/dataset/johanna_testdata/train'\n",
    "val_dir = 'data/dataset/johanna_testdata/val'\n",
    "test_dir = 'data/dataset/johanna_testdata/test'\n",
    "\n",
    "# constants\n",
    "img_size = 150\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "nb_train_samples = len(next(os.walk(train_dir))[2])\n",
    "nb_val_samples = len(next(os.walk(val_dir))[2])\n",
    "print(\"training samples:\", nb_train_samples)\n",
    "print(\"validation samples:\", nb_val_samples)\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing model with frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing weights and updating last layers\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 40,408,899\n",
      "Trainable params: 32,773,635\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hannes/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/6\n",
      " 95/848 [==>...........................] - ETA: 1:05:33 - loss: 6.4897 - acc: 0.5947"
     ]
    }
   ],
   "source": [
    "print(\"Freezing weights and updating last layers\")\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "\n",
    "#create model and freeze all but last conv block\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add VGG network and extra layer to model\n",
    "model = models.Sequential() \n",
    "model.add(vgg_conv)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# summerize and compile\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fine-tune model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps= nb_val_samples // batch_size)\n",
    "\n",
    "# Save model and plot accuracy \n",
    "#model.save('transfer_model.h5')\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "eps = range(len(acc))\n",
    "plt.plot(eps, acc, 'b', label='Training acc')\n",
    "plt.plot(eps, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "pred=model.predict_generator(test_generator)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing model with no frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vgg\n",
    "vgg_conv_2 = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "\n",
    "# add vgg network and extra layer to model\n",
    "model_2 = models.Sequential() \n",
    "model_2.add(vgg_conv_2)\n",
    "model_2.add(layers.Flatten())\n",
    "model_2.add(layers.Dense(1024, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#summerize and compile\n",
    "model_2.summary()\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_2 = model_2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nb_val_samples // batch_size)\n",
    "\n",
    "# Save the model\n",
    "#model_2.save('non_transfer_model.h5')\n",
    "acc = history_2.history['accuracy']\n",
    "val_acc = history_2.history['val_accuracy']\n",
    "eps = range(len(acc))\n",
    "plt.plot(eps, acc, 'b', label='Training acc')\n",
    "plt.plot(eps, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "pred_2=model.predict_generator(test_generator)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
